name,format,description
CNN Factored sEMG Based Limb Angle Prediction on the Edge - Advancing Model Predictive Control of Robotic Rehabilitative Systems,Poster Discussion,"Robotic rehabilitative systems promise to revolutionize the future of stroke rehabilitation. Controlling motors to allow the patient to move at their will during parts of the motion that they don’t require assistance is important for effective rehabilitation, especially at the later stages of therapy. 

However contemporary rehabilitative systems use a force-based feedback system that provides a small resistance to the motion as the system adapts only after the interaction between the patient and the machine. However, if the motion is predicted in advance, and further engineered in the rehabilitative system, these small resistive experiences are unfelt. 
Considering the critical method of usage and vulnerable segment of users, prediction on the edge to satisfy timing becomes imperative, besides running the process on constrained resources is expected. As a step towards this model predictive control approach, this paper proposes a limb angle prediction strategy based on surface Electromyography (sEMG) signals. The proposed strategy that is established from Convolutional Neural Network (CNN) is demonstrated and validated on the edge for upper limb bicep flexion and extension movement. 
The paper measures the impact of various design parameters on the implementation of the system and its relations to prediction accuracy, prediction time and resource requirement on the edge. A dataset comprising of sensory systems employed, and prediction models are made freely available for easy adoption and further usage to the researchers and designers community."
GAMLNet: A Graph Based Framework for the Detection of Money Laundering,Poster Discussion,"The accuracy of classification algorithms in detecting fraudulent financial activity is critical in assisting human analysts in the task of preventing financial crime. 
We consider financial transactions in the form of a directed graph and propose a Graph Neural Network (GNN) model for the identification of money laundering activity. Our method generates a set of structurally aware and statistically significant features for each graph node and utilizes them as input to the GNN classifier, that comprises of the combination of the layers of two recently proposed message-passing architectures. 
The effectiveness of our approach is demonstrated in experiments with synthetic data that simulate real-world behavior and are infused with seven anomalous money laundering topologies. The accuracy of our method is consistently higher than that of other GNNs and tree-based classification methods over datasets of increasing size and increasing imbalance between the fraudulent and benign classes."
Harnessing the Power of Forecasting: Best Practices & Use Cases,2 Business,"Forecasting is essential for business strategy, supply chain optimization, financial planning and more. Our presentation will showcase best practices in forecasting, drawing from our work with clients in the retail, pharma, and energy sectors. We’ll demonstrate how accurate forecasting can improve business, offering insights into the tools and techniques that enable us to predict and prepare for the future effectively. Join us to learn how to harness forecasting for your company’s competitive edge."
EdgeAI: Innovating at the Intersection of AI and Embedded Devices,2 Business,"In today's rapidly evolving technological landscape where Large Models are revolutionizing AI, EdgeAI emerges as the alternative opportunity to embed artificial intelligence directly into devices. This approach has several advantages over Large Models and, generally, cloud-based models: 
- Lower costs;  
- Lower delays and real-time inference;  
- Lower energy consumption (less computational power requires less energy);  
- Absolute privacy: data isn’t transferred over a network, it remains in a closed loop;  
- Problems are likely contained within a single device since it is decentralized, allowing for predominantly local but easier maintenance;  
- Thanks to the fast evolution of hardware capabilities, nowadays performances are comparable to those of large servers.  

However, the journey to harnessing the full potential of EdgeAI is not without challenges. Importing neural networks (NNs) onto embedded devices requires meticulous optimization to ensure efficient operation within hardware constraints. Processes such as distillation, pruning, and quantization play pivotal roles in tailoring NNs for embedded deployment, mitigating the performance gap while adapting to diverse device requirements. Furthermore, addressing hardware dependency is essential for ensuring cross-platform compatibility. 

To address optimization challenges in EdgeAI, Artificialy has implemented several key processes. First, distillation enables the creation of a more portable model by mimicking the results of a stronger teacher network, ensuring trustworthiness. Pruning further streamlines neural networks by removing branches and redistributing weights, optimizing efficiency. Additionally, training-aware quantization accounts for potential approximation by converting floating points to integers, enhancing performance while operating within hardware constraints. 

EdgeAI is essential when constant, real-time monitoring is required, such as in automotive, smart cities, or industrial automation. It also greatly benefits all applications that require maintaining high levels of privacy or confidentiality, since all data remains on the device. 

In our presentation we’ll show how we narrowed the gap between starting point and result, despite the great reduction in computational power. We will also review our case histories, with data on how many operations were avoided while maintaining same or very close accuracy in results of NNs on EDs vs. Servers. In particular, a Smart City solution developed with Sony for their IMX500 cameras, and a driver monitoring system that recognizes fatigue and distractions at the wheel, processing 30 FPS on NXP i.MX 8 Plus. 

Our team at Artificialy, doesn't just excel in EdgeAI; we're at the forefront of innovation across a spectrum of AI technologies: whether it's computer vision, forecasting and time series analysis, NLP, or process optimization, we’re proud to have proof of our expertise in the full gamut of AI applications. Since 2020, our exceptional team of data scientists and engineers delivers tailor-made solutions to businesses and institutions, spanning diverse domains. From pioneering EdgeAI solutions with Sony to revolutionizing manufacturing optimization built in Bystronic’s machines, all the way to our newly launched on-premise product PrivateGPT, we support our client’s needs from the assessment phase through to solution design, coding, packaging and scaled industrialization."
Crossing the Chasm Between Data Pilots and the Implementation of Data Products,2 Business,"[at] consults companies along their entire digital transformation journey: from the development of a data strategy to the optimization of AI and analytics applications, to the implementation of data products, and to regular operations. Let’s have a closer look at how we enable customers to make the decisive step from a data pilot to the implementation of a true data product in their organization."
Navigating Frontify's Journey to an AI-Ready Modern Data Stack,2 Business,"Join Michal, Frontify's Head of Data, as he shares their journey towards a modern data stack geared for AI integration. Facing an outdated infrastructure, Frontify adopted Fivetran, Snowflake, dbt, and ThoughtSpot to streamline operations. Learn how this stack accelerates data access, empowers dashboard creation, and liberates the team from manual tasks. Dive into the practicalities, including Snowflake Cortex for forecasting, and discover the transformative potential of a modern data infrastructure."
Driving Business Impact From Data,1 Leaders,"In this talk, we will share how together with our business partners, we identify, develop, and deliver data-driven use cases across the Migros Group. The talk will cover i) the strategic and structural context in which we operate, ii) outline the enabling and technological foundation that we are putting in place and iii) articulate with concrete examples our approach to value creation. Key takeaways of this talk include creating the data-driven impact roadmap, building the talent runway and scaling-up impact."
Increasing Trust in ML for Engineering With Explainable AI,2 Business,"Brückner Maschinenbau is the leading supplier of film-stretching machines in the world. Their machines produce film for a broad range of applications (e.g. food packaging etc.), by transforming raw polymer material (plastic) into extremely thin polymer film, with a throughput of several tons per hour. The produced film must meet certain optical, thermal, and mechanical quality metrics currently measured in a lab after production. This delayed feedback can cause a large amount of waste if quality metrics are not met. Furthermore, for polyethylene (PE, considered as the materiel of the future due to its recyclability in film stretching) the causal relationship between machine settings and film quality metrics are not yet fully understood. Thus, development of new film products may lead to a costly trial and error search until optimal machine settings are found. Thus, an AI based system is desired to predict the quality of the film given machine settings and materials used, during or even before production. 

Together with Brückner Maschinenbau, Zühlke built an ML model that predicts certain quality metrics of PE-film products using machine settings and material properties as features. Furthermore, we go beyond the standard ML workflow and ML methods by employing explainable AI techniques to shed light into the relationships between machine settings and predicted film quality. The gained understanding of model behavior enabled feedback from domain experts to further improve ML models and increased their trust in ML models. Moreover, Explainable AI can create data-driven insights about the stretching process and can guide product developers and machine operators towards optimal machine settings.  

The usage of Shapley values to explain model predictions will be the focus of our talk, as we are convinced that this will be of great interest to many data scientists in the audience. Explainable AI methods, such as Shapley values, explain how the input to a model affects the output of a model. Such methods are commonly used by data scientists to provide transparency into models and increase trust in AI, and further to address important societal challenges like AI fairness and the EU’s right to explanation. In this project, we use Shapley values to better understand the stretching process, and to potentially guide product development. However, to detect insights about the stretching process (i.e., which change in machine settings leads to better film quality) one must be cautious. ML models are based on correlation, and explainability methods like SHAP can be misleading when features are correlated. We use an heuristic approach inspired by ideas from recent research (including the causality graph of the stretching process and conditional probability distributions of the data) to make Shapley values more likely to discover insights about the stretching process that can be used optimize film quality. 

Our talk highlights how Shapley values can increase trust in AI and provides an overview of relevant methods as well as in-depth insights for data science practitioners."
Shaping Pharma Data and Analytics Strategy,1 Leaders,"Legacy data systems and siloed analytics capabilities hinder innovation within the pharma commercial sector. This presentation details Roche's journey in establishing a centralized, cloud-based data and analytics platform. 
The focus is on the strategic decommissioning of legacy systems, enabling business intelligence and machine learning use cases to accelerate business impact. The talk highlights the critical interplay of people, processes, and technology, offering a blueprint for successful cloud migration and data-driven transformation."
GenAI: Developing and Deploying a Specialized Underwriting AI Assistant,2 Business,"Reinsurance companies provide expert medical and underwriting knowledge to insurance companies by distributing Underwriting Manuals. Swiss Re's manual, Life Guide, receives over 23 million hits per year from Life & Health Underwriters (UWs). It provides them with guidance to review a Life or Health insurance application, helping them understand and translate risk into rating adjustments for each individual applicant. Taking the right decision in a timely manner becomes a challenge due to the vast diversity in medical records, history, and profiles of the applicants. In this talk, we present a novel add-on to Life Guide that provides the ability to interact in a human-like fashion, akin to having a specialized assistant by one's side. We cover the major steps needed for creating such a system: from development all the way to productization. To our knowledge it is the first attempt to integrate an advanced chat interface with an underwriting manual in front of UWs for actual decision making.

We start by describing the core of our solution made of a Retrieval-Augmented Generation (RAG) system leveraging state-of-the-art Large Language Model (LLM), namely GPT-4. A crucial first step consists in properly understanding and structuring the Underwriting Manual. This enables us to implement a precise chunking strategy whereby the Manual is split into text chunks and finally embedded into a vector space and stored for later retrieval. We then showcase our information retrieval system used to bring relevant context to the LLM for the final generation of an answer. We give the rationale behind our technical choices and address the specific case of tabular data. 

In a second stage, we cover the actual deployment of the solution in front of underwriters. More than the description of a robust production setup, we present our attempts at solving issues specifically related to the use of the RAG technology: adapting to changing LLM versions over time, availability, cost, and latency optimization as well as text chunks management. Besides engineering aspects, we also describe our attempt at creating a trust-inducing User Experience and User Interface despite the hallucinatory nature of LLMs. 

From there, we tackle the issue of evaluating our system. We suggest and report on qualitative assessments as well as quantitative metrics gathered from exposing the application to Swiss Re UWs. For instance, we reached a top-5 accuracy of over 90% in the information retrieval step on so-called information pages of the manual.

We finally take a step back to examine the potential impacts of such a system on underwriting practices. We argue that this new interface with Underwriting Manuals has the potential to redefine how UWs interact with them. We also emphasize the importance of mitigating known limitations of LLMs thanks to all available means: from proper evaluation methodologies all the way to User Interfaces and User Experiences."
A Data-Centric-AI Trick to Clean Your Dirty Data,3 Scientific,"Detecting unusual or abnormal patterns in data is one of the common tasks of AI algorithms in commercial applications. In some applications, such as fraud detection, defect detection or medical diagnostics, anomaly detection is the main objective. In other applications, detecting abnormal data points is part of the data cleaning and preparation pipeline. In all cases, the use of AI-based methods relies on having a training dataset which can represent the normal behaviour, and must therefore be free of anomalies. 
Problems arise when we realize that having an anomaly-free training dataset is not always possible in practice: most real-world datasets are contaminated with unknown anomalies or mislabeled data. 

In my talk, I will discuss this challenge and present a simple AI-based trick that provides a solution to the contaminated training data problem in a fully unsupervised manner, with no need for labels or prior information about the data. The framework is generic in nature, independent of the specific field of application, be it finance, cybersecurity, manufacturing or medicine. Moreover, it allows for a fast integration with any existing machine-learning algorithm for anomaly detection. The effectiveness of the method will be demonstrated on time-series sensor data from different types of machines."
Navigating Cybersecurity Challenges and Opportunities in the Age of AI: A Focus on Large Language Models by Hackers and Defender,2 Business,"In the digital age, the convergence of artificial intelligence (AI) and cybersecurity has grown increasingly intricate. This discussion delves into the dual role of AI in intensifying and alleviating cybersecurity threats.
The emergence of AI has catalyzed transformative shifts across diverse sectors. AI tools, employed for a spectrum of tasks from data analysis to predictive modeling, are redefining conventional practices. Concurrently, the advent of Large Language Models (LLMs) introduces substantial cybersecurity challenges. Hackers are progressively exploiting sophisticated AI-driven techniques to infiltrate security systems, thereby posing threats to data privacy and integrity.
The potential of LLMs in orchestrating tasks is often underestimated. They usher in a new level of automation, fostering innovative problem-solving approaches. This is a double-edged sword. On one hand, defenders can significantly reduce their threat identification and remediation time, bolstering cyber resilience. Conversely, hackers may exploit LLMs to uncover new avenues for system control. In this landscape of cyber warfare, patching vulnerabilities transitions from an option to a necessity for survival."
GenAI Based Automated Email Response System - Challenges With Production Systems,2 Business,"Generative AI has numerous applications in the area of customer service. In this talk, we will discuss the development and implementation of an automated email response bot using GenAI. 
We will explore how GenAI can be used to understand customer emails, extract order information, query ERP systems, and generate response emails. We will also delve into the challenges faced in connecting GenAI systems with databases and extracting order information. Finally, the process and challenges of introducing and productionalizing AI-based systems into everyday business are also discussed. Join us to learn about the potential benefits and challenges of using GenAI in email response automation."
Unlocking Success: A Strategic Guide from Data to Value,1 Leaders,"Join us for an insightful session with Raiffeisen, Switzerland’s second-largest bank, as we delve into the strategic relevance of data and analytics in banking. Imagine transforming your numerous proof of concepts into scalable analytics that yields impressive outcomes! Learn from a real-life example the strategic importance of consistently broadening your focus, nurturing your data asset, and fortifying your team with each new use case. And comprehend the necessity of prioritizing your data asset over technology to triumph in your next pitch. We’ll also converse about the advantages of vertical integration to deliver outstanding results, drawing comparisons with prosperous companies like Tesla and Apple. Seize this chance to unleash the potential of your own data assets and skills."
Artificial Intelligence - A Suitable Tool for Mitigating the Risks of Climate Change?,2 Business,"Climate Change exposes the Swiss society and industry with challenges such as more intense and more frequent natural hazards, disrupted and uncertain supply chains but also with new regulatory requirements. Almost all sectors in Switzerland are impacted by climate change directly or indirectly. Consequently, there is a strong need for adaptation in these sectors and a stringent necessity for mitigation in combination with e.g., rules, regulations, or normative pressure. Beyond the obvious physical climate disaster risks, this bears for stakeholders administrative, legal, and other economic risks

In this talk, we want to present how the emerging technologies of artificial intelligence (AI) can support Swiss stakeholders - from government over industry to the public. We will present the availability of these technologies, their use and how to draw value from them. One focus here is on the so-called transition risks related to climate change, which are critical for organizations that need to manage the transition to a more sustainable, resilient, and lower-carbon economy. We will show, why transition risks are multi-faceted and arise from the need to adapt to evolving climate change mitigation policies, technological advances and changing consumer preferences. In this context, we present the potential of AI to deal with the most significant challenges, including potential fluctuations in the value of assets, the emergence of stranded assets due to unforeseen or premature depreciation and shifts in operating costs. This is primarily analysed for changes which are inseparably linked to a company's carbon emissions, as efforts to reduce these emissions often require significant changes in business operations and strategies. Findings from the current White Paper study “AI to mitigate climate-change impact on Swiss society and economy” are presented, including the challenges and possible solutions around: i) data discovery and accessibility, ii) machine learning at scale, and iii) services.

We will discuss opportunities how to empower Swiss stakeholders to become more sustainable through quantitative climate impact and transition-risk assessments, whereas many of these opportunities to enhance climate resilience and sustainability are based on latest advancements in Earth observation and artificial intelligence."
Scan to BIM: Deep Learning Techniques for Automatic Reconstruction of Indoor Environments,2 Business,"The development of smart cities and digital twins has amplified the need for detailed and accurate 3D models of indoor environments, which are essential for applications such as indoor mapping, navigation, intelligent building management, and virtual reality. Building Information Modeling (BIM) has become increasingly important in representing the physical and functional characteristics of building interiors, with LiDAR technology playing a crucial role in capturing large indoor scenes for this purpose. 

In this talk we present applications, techniques and key challenges to model reality from 3D point cloud data. 

The automatic reconstruction of complete volumetric and functional interior models from LiDAR point clouds presents significant challenges. These include the need for accurate models to replace labor-intensive manual modeling, the interpretation of how to model complex real environments with wall orientations that are not aligned with orthogonal coordinates, variable room heights or non-cuboidal wall shapes, and technical difficulties such as incomplete point cloud scans, scanning shadows, complex wall shapes, and variations in point cloud density. 

Several techniques have been explored and developed at Hexagon to address these challenges: Top-down approaches, such as floor plan reconstruction from 2D processing of point cloud density maps, offer structured representations but may struggle with non-Manhattan scenes and complex geometric primitives of structural elements such as walls. Bottom-up approaches involve 3D point cloud segmentation followed by the modeling of primitives, which can capture the complexity of indoor environments but may not provide complete and accurate models. Combined approaches, such as those using 3D object detection or instance segmentation, integrate the strengths of both top-down and bottom-up methods for improved modeling and robustness.

Looking ahead, there is potential for the fusion of images and point cloud data, utilizing multi-modality models that incorporate both, 2D and 3D information. New 3D multi-modal model architectures, that accept point cloud data or depth map as inputs, can enhance the robustness, accuracy and detail of detection and reconstruction of components like doors and windows. Open vocabulary and advanced 3D scene understanding techniques also hold promise for more dynamic and adaptable modeling and direct user interaction with the digital model. Generative AI offers the chance to create new synthetic indoor scene environments, enriching existing digital realities with additional variation, advanced visualization or for training deep learning 3D modelling pipelines. 

In conclusion, the field of automatic indoor model reconstruction from LiDAR point cloud data is advancing rapidly, with deep learning techniques at the forefront of addressing the challenges and improving the accuracy and robustness of BIM models. The integration of various data modalities and the advancement of machine learning algorithms hold great potential for the future of indoor mapping and navigation."
Secured UAV Navigation: A Novel Intrusion Detection System Based on PWM Signal Analysis,3 Scientific,"Unmanned Aerial Vehicles (UAVs) have seen exponential growth in applications, from surveillance to logistics. Ensuring their security, especially in a Global Position System (GPS)-compromised environment, is critical. This paper introduces an Intrusion Detection System (IDS) as a countermeasure against sensor attacks that leverages Pulse Width Modulation (PWM) signals, generated by the UAV Board. 
The IDS, positioned at the Electronic Speed Controllers (ESCs), can detect anomalies even when conventional ground station detections fail. Multiple unsupervised machine learning algorithms were evaluated for this purpose. The Local Factor Outlier (LOF) emerged as the most accurate with 75.87%, closely followed by One Class SVM (OCSVM) at 74.17%, and Isolation Forest (IF) at 71.59%.These findings suggest that monitoring PWM signals could provide a novel layer of security for UAVs, making them resilient against certain forms of cyber-attacks."
A Lightweight Data Mining Platform for Dynamic and Reproducible Malware Analysis,3 Scientific,"In the era of digitalization, the availability of data is paramount for any scenario that requires informed decision-making. In the cybersecurity world, this is no different. This is especially the case for malware since, even though malware samples share common ancestors, implementations are commonly adapted into many strains, requiring frequent execution and analysis to implement appropriate detection and mitigation mechanisms based on malicious patterns.
Sandboxes have emerged as an environment to dynamically execute and analyze malware. However, existing platforms lack real-time, interactive, reproducible malware analysis. Since they do not explore the applicability of container-based isolation, this paper proposes SecBox. To extract system calls, performance metrics, and network traffic, solution* implements a real-time, visual, easy-to-use tool to execute malware from existing sample exchanges. The platform explores the suitability of lightweight, container-based sandboxing. Based on multiple experiments, solutions achieves good results in terms of performance, isolation, reproducibility, and monitorability of malware."
Automated Process Monitoring in Injection Molding via Representation Learning and Setpoint Regression,3 Scientific,"Online process monitoring is essential to detect failures and respond promptly in automated industrial processes such as injection molding. Traditional systems rely on experienced operators manually defining operational boundaries around a reference signal. We propose a data-driven representation that auto-tunes the sensitivity to a pre-set specificity threshold and automatically detects anomalies alongside interpretable indices that help identify root causes. Our automated system achieved an average AUC of 0.998 and detected 100 percent of the anomalies with the proposed dynamic calibration of the data-driven embedding method. The dynamic calibration, which accounted for drift, boosts the average specificity from 0.362 to 0.869. 
The outputs also indicate the direction and relative magnitude of characteristic deviations caused by machine parameters, including holding pressure, mold temperature, and injection speed. The AI-derived process boundaries are superior to manual annotation in tested real-world production environments."
Towards Reliable Evaluation of Large Language Models (LLMs),3 Scientific,"Large Language Models (LLMs) have become ubiquitous in today's technology landscape due to their remarkable ability to ""understand"" and generate human-like text. They are used in a wide array of applications from chatbots to content creation. But how do you properly evaluate the quality of such models? 
This talk gives an overview of current approaches to evaluating LLMs and their respective shortcomings. We then present a statistical framework, developed by researchers at the ZHAW Datalab, to determine how reliable an evaluation method is, and how much data - human-annotated vs. automatically generated - is needed. We then show how this framework can be used to implement trustworthy real-world evaluation settings for LLMs."
Towards the Certification of AI-Based Systems,3 Scientific,"Certifying the trustworthiness of Artificial Intelligence (AI)-based systems based on dimensions including reliability and transparency is crucial given their increased uptake.
Likewise, as regulatory requirements are established, actionable guidelines for certification will be useful for developers and certification bodies to ensure trustworthiness of AI.
Here, we present an ongoing effort to develop a validated AI certification scheme which is a framework for assessing the trustworthiness of AI systems including specific objectives with their corresponding means of compliance (i.e. process, documentation or technical methods).
Importantly, the scheme makes an explicit link between legal requirements and validated techniques for assessing the compliance of AI systems, resulting in the implementation of a workflow to support AI certification.
We explain the rationale for developing the certification scheme and demonstrate the assessment of an example use case with a concrete workflow traversing from objectives to corresponding means, focused on reliability and transparency."
Rise of AI: Reshaping the Insurance Industry,1 Leaders,"AI and GenAI are with no doubts a true game-changer in the insurance industry. Insurance customers expect protection in any of their lifetime situations, at any place, and that this protection is given at a fair premium. To respond to those customer needs, we believe three things need to come together: high-impact use cases, a mechanism to scale AI capabilities to any place where our customers are, and an effective AI governance that ensures proper risk mitigation. 
In his presentation, Christian will highlight where Zurich Insurance is on its journey and what the learnings have been so far."
The Power of Visual Analytics: Combining Data Analytics and Visualization for Exploring Big Data,Keynote,"Never before in history has data been generated and collected at such high volumes as it is today. For the analysis of large data sets to be effective, it is important to include humans in the data exploration process and combine the flexibility, creativity, and general knowledge of humans with the enormous storage capacity and computational power of today's computers. Visual Analytics helps to deal with the flood of information by integrating humans in the data analysis process, applying its perceptual abilities to large data sets. Presenting data in an interactive, graphical form provides effective ways to understand and analyze large data sets, allowing novel discoveries and empowering individuals to take control of the analytical process.

The talk presents the potential of visual analytics and discusses the role of automated versus interactive visual techniques in dealing with big data. A variety of application examples ranging from customer feedback analysis over network security to sports analytics illustrate the exciting potential of visual analysis techniques but also their current limitations."
Predictive Maintenance & Time Series Anomaly Detection With Darts,Workshop,"Are you tired of reactive maintenance? Do you want to learn how to predict equipment failures before they happen? Detect abnormal patterns in your time series. Join our hands-on workshop on predictive maintenance and anomaly detection using Darts - the open-source library for time series forecasting and anomaly detection (Python).
In this workshop, you will learn how to use time series data to predict equipment failures and detect anomalies. First, we cover the basics of time series forecasting, from data pre-processing & feature engineering to model selection. Then, we move to two real-world use cases from the healthcare and industrial sectors.
By the end, you will have a good understanding of how to perform both time series forecasting and anomaly detection for predictive maintenance and other applications.
This workshop is suitable for data scientists, machine learning engineers, and anyone interested in time series analysis. Basic knowledge of Python is recommended."
Grow Your ML Garden on Google Cloud,Workshop,"Grow Your ML Garden on Google Cloud"" is a hands-on workshop that invites attendees to delve into the world of Machine Learning Operations (MLOps) using Google Cloud Platform (GCP) as a fertile ground for AI development. This workshop will guide participants through the lifecycle of ML projects - from preparing the initial data soil to nurturing and cultivating robust ML models. Emphasizing on MLOps practices, this workshop covers three key areas: preparing the data in a BigQuery's feature store, developing ML models with Vertex AI, and managing a sustainable MLOps environment using Vertex AI Pipelines. 

The participants will have the opportunity to predict outages of wind turbines using a real world dataset with time series data."
Tutorial: Fostering Integrity in AI - Embracing Ethical Principles and Explainability,Workshop,"Artificial Intelligence (AI) is rapidly transforming our world, offering tremendous potential to improve efficiency, productivity, and decision-making across various industries. However, AI systems' increasing complexity and pervasiveness raise concerns about their ethical implications and the need for transparency. This tutorial will provide a comprehensive overview of the principles and practices for fostering integrity in AI development and deployment. The tutorial will consist of five presentations, a practical hands-on session, and a panel discussion with experts from industry and academia."
AI Standards: Navigating Compliance and Regulation for Responsible AI,Workshop,"AI-related standards are a key element to comply with upcoming regulations of AI. This workshop will provide an overview of the regulation process and important standards for AI innovation, and provide practical exercises on how to apply them in practice."
Generative AI for Well-Being,Workshop,"Generative AI has reached broad attention in the media over the last months. Different new use cases have been identified to support people in their daily work and make their work more efficient. But what about the well-being of the individuals?
Different studies have shown that there is a rise of stress, also in Switzerland. The new technologies bring potential for applications to help people to fight stress and increase their well-being. For example, chatbots or coaching technologies can support mental health or therapy in the setting of blended therapy. On the other side, there is a huge potential of multimedia interventions for elderly people, patients or stressed workers. 
In this interdisciplinary workshop, we explore what different directions are possible and how these latest technologies can be applied for the well-being of humans. The workshop consists of a series of talks, and a hands-on part."
Building Agile Data Foundations for Data Mesh & Generative AI,Workshop,"Our workshop, ""Building Agile Data Foundations for Data Mesh & Generative AI,"" is designed to provide participants with the skills necessary for implementing Data Mesh and leveraging GenAI. Centered around the Data Vault 2.0 methodology, it teaches a flexible and scalable approach to data modeling, essential for adapting to the evolving needs of GenAI and Data Mesh frameworks. This approach is not only crucial for managing complex data systems but also for powering robust and safe LLM applications. While the concepts are demonstrated on Google Cloud Platform, they are applicable across various cloud platforms, underscoring the versatility of Data Vault 2.0. 
Participants will also gain practical experience with dbt (data build tool), learning to create agile, maintainable data transformations. This workshop equips attendees with the knowledge to effectively manage complex data systems and develop powerful LLM applications, crucial for thriving in a data-driven future."
A Hands-on Exploration of the Azure OpenAI API,Workshop,"This workshop offers an invaluable opportunity for individuals and companies seeking to enhance their workflows by strategically incorporating natural language processing (NLP) capabilities into their data infrastructure. Participants will gain insights into real-world applications and learn how to effectively integrate Azure OpenAI API to tackle industry-specific challenges. 
Moreover, key topics including model enhancement techniques such as prompt engineering, RAG, and Fine-Tuning will be covered. Additionally, we will be joined by Microsoft representatives who will address governance and security considerations, ensuring a comprehensive understanding of these crucial aspects when using Azure OpenAI Service. The skills acquired in this workshop are transferrable to the utilization of the OpenAI API."
Data Science in Spatial Computing - Explore Your Data Using PlotAR,Workshop,"Did you ever dream about fully immersing yourself in your data, walking through it, grasping it with your hands? Learn with some hands-on exercises the basics of PlotAR.
If you bring your own data there will be time to start exploring it during the workshop."
Unlocking the Power of LLMs: From Prompt Engineering to Agents,Workshop,"Unlock the full potential of large language models! Dive into the art of Prompt Engineering to harness the power of ChatGPT in boosting productivity across domains by employing various strategies. For instance, ChatGPT as a programming assistant, how to use it in automation activities and speed up daily office’s activities.  
We will then journey into the secret lives of LLM Agents, overcoming data limitations through the use of external tools and services. This part of the hands-on will be dedicated to a detective-themed puzzle/case designed to perform a simple Extract-Transform-Load (ETL) operation with multi-modal inputs through simple text prompts by connecting the agents to Azure Cognitive Services. The hands-on sessions will cater to both technical and non-technical participants."
Next-Gen Cleantech Solutions: Mining Insights From Media and Patent Data With Natural Language Processing and Large Language Models,Workshop,"At a time when tackling environmental challenges is of paramount importance, the cleantech industry plays a central role in promoting sustainable solutions. However, technological innovation in the cleantech sector requires a deep understanding not only of the technologies but also of the market requirements. This information is usually embedded in a large amount of patent and media data, which is difficult to analyze manually to effectively capture the development trend. Using Natural Language Processing (NLP) and the latest advancements in Large Language Models (LLMs) is a natural choice to accelerate innovation. 
In this workshop, we will share our insights gained in solving this task. Several presentations on various relevant topics will be offered, followed by a hands-on session where participants can try out our LLMs-powered cleantech question-answering and recommendation system."
Elevating Business Through Social Values – The Companies’ Perspective and Future Challenges,Workshop,"Sustainable business practices focus on considering the impact on the environment, society, and the economy. While economic and ecological sustainability are already firmly anchored in practice, the social dimension of sustainability tends to be neglected. On the one hand, the spectrum of social outcomes is very broad and covers various areas such as diversity, inclusion, transparency and working atmosphere. On the other hand, there are various data-driven support services available to foster these outcomes potentially, but a link between them is missing. 

It is therefore becoming increasingly important for companies to focus on social values. They should understand the causal relationship between data-driven operations and social outcomes, improve them, and promote their social commitment."
Real World Applications of LLMs for Business and Industry,Workshop,"This workshop aims to delve into the real-world applications of Large Language Models (LLMs), targeting both technical and business professionals. The agenda includes a balanced mix of hands-on coding and brainstorming sessions to apply LLMs in addressing modern business challenges. Participants will start with a comprehensive introduction to LLMs, understand a real business case, and then move into developing a web application using LLMs. Subsequent sessions focus on performance and cost analysis, optimizing solutions, and exploring various applications. By the end, attendees will gain practical skills and strategies for using LLMs efficiently in diverse scenarios."
Web Crawling at Scale,Workshop,"Unlock the potential of web crawling with our comprehensive 3-hour workshop! This hands-on tutorial will lead you through the fundamental aspects of web scraping, covering everything from understanding its use cases to overcoming challenges and deploying your own web crawlers. Our experts will showcase a range of successfully implemented web crawling projects across various industries. Additionally, our experienced mentors will provide hands-on assistance with coding exercises in Jupyter Notebooks Environments hosted on the cloud. 
Whether you're a beginner or have some experience, this workshop caters to various levels of expertise."
Fair ML - The Challenges and Opportunities of Integrating Fairness Into Machine Learning Applications,Workshop,"Ethics and fairness of Machine Learning applications are becoming increasingly important in the world of Data Science. Widespread use of ML applications, such as recommendation systems or large language models call for methods that ensure that they comply with ethical standards and policies.
This workshop offers the chance to experience hands-on how to deal with biased data leading to biased models. We will highlight both the challenges of incorporating fairness into ML applications, as well as the opportunities that modern fairness tools offer in removing bias. This workshop targets ML practitioners and executives alike as incorporating fairness is both a technological as well as a societal challenge."
AI in Action: A Practical Guide to Using Agents for Integrating Custom Knowledge With Large Language Models,Workshop,"This workshop explores Retrieval Augmented Generation (RAG) in Large Language Models (LLMs), focusing on the integration of custom knowledge bases and the use of agents in the generation process. Attendees will learn about the two main components of RAG: retrieval and generation. The retrieval process involves extracting relevant information from structured and unstructured data using methods like text embeddings and additional algorithms of information retrieval. In the generation phase, the model generates contextually appropriate responses by conditioning the LLM on both the input query and retrieved information. 
The workshop also delves into the role of agents, which are managed by an orchestrator that delegates tasks and condenses results into valuable answers for the user. By combining RAG with agents, the workshop aims to enhance the performance of generative models and produce more informed and relevant outputs."
AI Developer Workshop,Workshop,"Discover how generative AI enables businesses to develop better products and services and deliver original content tailored to the unique needs of customers and audiences. At the second part, the attendees will interact with and prompt engineer LLaMA-2 models to analyze documents, generate text, and be an AI assistant."
