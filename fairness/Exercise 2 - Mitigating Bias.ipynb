{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Workshop exercise 2 - Mitigating Bias**"],"metadata":{"id":"lbHqX5IOilxd"}},{"cell_type":"markdown","source":["## **Purpose**\n","\n","The scope of this exerecise is to quantify bias and subsequently to mitigate it to obtain a fair algorithm.\n","Before we begin with the analysis we need to set up our environment and install required libraries"],"metadata":{"id":"pKTosztJiq5m"}},{"cell_type":"markdown","source":["## **Libraries**\n","\n","We will start by running the following cell to install and import some necessary."],"metadata":{"id":"nrK5pDMqniaK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EwkZwVdDM7v"},"outputs":[],"source":["# Install the fairlearn library\n","!pip install fairlearn\n","\n","# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set()"]},{"cell_type":"markdown","source":["## **Import the data**\n","\n","We proceed by importing the COMPAS dataset into a pandas dataframe."],"metadata":{"id":"IiqwYl10nhLD"}},{"cell_type":"code","source":["# Load the dataset\n","url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n","\n","# Read the data into a pandas dataframe\n","data = pd.read_csv(url)\n","\n","# Select the appropriate columns needed for our analysis\n","columns = [\"sex\", \"age_cat\", \"race\", \"v_score_text\", \"c_charge_degree\", \"priors_count\"]\n","data = data[columns]\n","\n","# Display the first few rows of the dataframe\n","data.head(10)"],"metadata":{"id":"073yuUIUN3zt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To build our ML model, we will use the following set of features:\n","\n","- **sex**: Gender of the individual (e.g., Male, Female)\n","- **age_cat**: Age category of the individual (e.g., 25-45)\n","- **race**: Racial background of the individual (e.g., African-American, Caucasian, Hispanic)\n","- **priors_count**: Number of prior offenses\n","- **c_charge_degree**: Degree of the current charge (e.g., Felony, Misdemeanor)\n","\n","Our target (label) will be:\n","- **score_text**: Textual description of the COMPAS risk score (e.g., Low, Medium, High)"],"metadata":{"id":"wbtzKW1Ej6if"}},{"cell_type":"markdown","source":["## **Preparing the data**\n","\n","Next, we prepare the data for further processing. Notably, we perform the following steps:\n","* Drop samples cassified as \"Medium\" risk because we want to focus on the \"High\" and \"Low\" risk categories.\n","* Focus on the races \"African-American\" and \"Caucasian\" due to the low representation of the other races."],"metadata":{"id":"lGAiOd5KoAYG"}},{"cell_type":"code","source":["# Drop the medium risk records\n","data = data.drop(data[data.v_score_text == \"Medium\"].index)\n","\n","# Only keep the races \"African-American\" and \"Caucasian\"\n","data = data.loc[data['race'].isin([\"African-American\", \"Caucasian\"])]\n","\n","data.head(10)"],"metadata":{"id":"N41DfHS2DkV4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Building our ML model**\n","\n","We will now build a simple ML model and treat \"race\" as our protected variable. The purpose of the model is to predict whether a defendant has a high or low risk of recidivism. We need again to follow some steps.\n","\n","* Encoding of the risk score column\n","* Encoding of race column\n","* Encoding of other columns\n","* Split of the dataset in training and test set (we will use a 80-20 split)\n","* Train a classifier on our (training) data\n","* Compute our accuracy on test data\n","* Calculate demographic parity difference\n","* Calculate equalized odds difference\n","\n","\n"," Let's train it and see if we can identify any biases in our predictions."],"metadata":{"id":"mvmCnJl9o0yh"}},{"cell_type":"code","source":["# Encoding the risk score to a numerical value\n","mapping = {\"Low\": 0, \"High\": 1}\n","data = data.replace({\"v_score_text\": mapping}).rename(columns={\"v_score_text\": \"high_risk\"})\n","data.head(10)"],"metadata":{"id":"CW7XNAmvFPLW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import LabelEncoder\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Label encode 'race' column\n","le = LabelEncoder()\n","data['race'] = le.fit_transform(data['race'])\n","y = data['high_risk']\n","\n","# One-hot encode other categorical columns, but exclude 'race'\n","X = pd.get_dummies(data.drop(['high_risk', 'race'], axis=1), drop_first=True)\n","X['race'] = data['race']\n","\n","# Split the data into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","\n","# Train a simple machine learning model (Random Forest Classifier)\n","model = RandomForestClassifier(random_state=42)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)"],"metadata":{"id":"_maKZNCsPgPm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have trained our classifier, we can evaluate it with respect to model performance and fairness metrics."],"metadata":{"id":"H8BBshJ8lNWF"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score\n","from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n","\n","# Calculate the accuracy of the baseline model\n","baseline_accuracy = accuracy_score(y_test, y_pred)\n","\n","# Calculate demographic parity difference\n","dp_difference = demographic_parity_difference(y_test, y_pred, sensitive_features=X_test['race'])\n","\n","# Calculate equalized odds difference\n","eo_difference =  equalized_odds_difference(y_test, y_pred, sensitive_features=X_test['race'])\n","\n","# Print results\n","print(f\"Baseline Model Accuracy: {baseline_accuracy:.4f}\")\n","print(f\"Demographic Parity Difference: {dp_difference:.4f}\")\n","print(f\"Equalized Odds Difference: {eo_difference:.4f}\")"],"metadata":{"id":"KjspH-l9lVMe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Question:***\n","*How would you interpret the results for Demographic Parity Difference and Equalized Odds Difference?*"],"metadata":{"id":"0GUjK-_moNk3"}},{"cell_type":"markdown","source":["**Explanation of metrics**\n","\n","- *Baseline Model Accuracy*: 0.8773 (or 87.73%): This tells us that your model correctly predicted the risk score for approximately 87.73% of the instances in our test set.\n","\n","\n","- *Demographic Parity Difference:  0.1319*: is a fairness metric that measures the difference in the rates of positive predictions (in this case the high-risk class) between two groups. In this case, we are looking at the race attribute, so the result suggests there is a difference in the rate at which African-Americans and Caucasians receive high-risk predictions.\n","A value of 0 would indicate perfect demographic parity, meaning both groups receive positive predictions at the same rate.\n","- *Equalized Odds Difference 0.3571*: Specifically, it calculates the largest disparity between either the True Positive Rates or the False Positive Rates for these groups. A value of 0 for the Equalized Odds Difference would indicate perfect fairness, meaning that both African-Americans and Caucasians have identical rates of true positives and false positives. However, our current value of 0.3571 suggests there's some inequality in the model's predictions for African-Americans versus Caucasians.\n","\n","- While our model performs reasonably well with an accuracy of 87.73%, there are concerns related to fairness, particularly concerning the race attribute. **That we are analysing today. The model is not treating both races equally in terms of positive predictions and correct positive predictions. These disparities can be a result of underlying biases in the training data, imbalances in sample sizes between groups, or other factors.**"],"metadata":{"id":"ToCe2AXWpyeM"}},{"cell_type":"markdown","source":["Let's compute some more key metrics of our classifier:\n","\n","**Precision:** What proportion of positive predicitons was acutally correct?\n","\n","$$\n","\\text{Precision} = \\frac{TP}{TP + FP}\n","$$\n","\n","**Recall:** What proportion of actual positives was identified correctly?\n","\n","$$\n","\\text{Recall} = \\frac{TP}{TP + FN}\n","$$\n","\n","**Selection Rate:** What proportion of the samples received a positive predition?\n","$$\n","\\text{Selection Rate} = \\frac{TP + FP}{P + N}\n","$$\n","\n"," *Note: Demographic parity difference is the absolute difference between the selection rates*"],"metadata":{"id":"Z7DMyZCipdKM"}},{"cell_type":"code","source":["from fairlearn.metrics import MetricFrame\n","from fairlearn.metrics import selection_rate\n","\n","race = X_test['race']\n","gm = MetricFrame(metrics=accuracy_score, y_true=y_test, y_pred=y_pred, sensitive_features=race)\n","\n","\n","metrics = {\n","    'precision': precision_score,\n","    'recall': recall_score,\n","    'selection_rate': selection_rate\n","}\n","\n","metric_frame = MetricFrame(metrics=metrics,\n","                           y_true=y_test,\n","                           y_pred=y_pred,\n","                           sensitive_features=race)\n","\n","ax = metric_frame.by_group.plot.bar(\n","    subplots=True,\n","    layout=[1, 3],\n","    legend=False,\n","    figsize=[15, 6],\n","    title=\"Show all metrics\",\n",")\n","# Remap the labels\n","label_mapping = {0: 'African-American', 1: 'Caucasian'}\n","\n","# Update x-tick labels for each subplot\n","for subplot in ax[0]:\n","    labels = [item.get_text() for item in subplot.get_xticklabels()]\n","    new_labels = [label_mapping[int(label)] if label.isdigit() else label for label in labels]\n","    subplot.set_xticklabels(new_labels, rotation=45)\n","\n","plt.tight_layout()"],"metadata":{"id":"xFPn-iECH2JQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Question***\n","\n","- What is the underlying assumption about the data when evaluating demographic parity?\n","- Why may this not not be justified here?"],"metadata":{"id":"zCAjogMax--g"}},{"cell_type":"markdown","source":["Let's look at some more metrics and how they differ between the races."],"metadata":{"id":"puy1c1sjyCta"}},{"cell_type":"code","source":["from fairlearn.metrics import false_positive_rate, true_positive_rate\n","\n","metrics = {\n","    'true_positive_rate': true_positive_rate,\n","    'false_positive_rate': false_positive_rate,\n","    'selection_rate': selection_rate\n","}\n","\n","metric_frame = MetricFrame(metrics=metrics,\n","                           y_true=y_test,\n","                           y_pred=y_pred,\n","                           sensitive_features=race)\n","\n","ax = metric_frame.by_group.plot.bar(\n","    subplots=True,\n","    layout=[1, 3],\n","    legend=False,\n","    figsize=[15, 6],\n","    title=\"Show all metrics\",\n",")\n","\n","# Remap the labels\n","label_mapping = {0: 'African-American', 1: 'Caucasian'}\n","\n","# Update x-tick labels for each subplot\n","for subplot in ax[0]:\n","    labels = [item.get_text() for item in subplot.get_xticklabels()]\n","    new_labels = [label_mapping[int(label)] if label.isdigit() else label for label in labels]\n","    subplot.set_xticklabels(new_labels, rotation=45)\n","\n","plt.tight_layout()"],"metadata":{"id":"w0pX50sywUR_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" *Note: Equalized Odds difference is the largest absolute difference between the TPRs and FPRs.*"],"metadata":{"id":"5ZbIgSmbz7gC"}},{"cell_type":"markdown","source":["## ***Question***\n","\n","- What is the implication of the difference in true-positive-rates and false-positive-rates with respect to racial fairness?\n","- Why may the equalized odds difference be misleading in some scenarios? What does it fail to capture?"],"metadata":{"id":"HNzvLzEdyMEt"}},{"cell_type":"markdown","source":["## **Bias Mitigation Techniques**\n","\n","In this part we will try to mitigate the bias in our predictions. For this we will try three methods: the simple \"fairness through unawareness\", the Exponentiated Gradient method to mitigate bias, and a threshold optimization technique."],"metadata":{"id":"-KJ362WPspkJ"}},{"cell_type":"markdown","source":["# 1.  Fairness through unawareness\n","\n","\n","In the following cell we will try to mitigate the bias in the dataset by using the most simple approach that is usually called **Fairness through unawareness** and it involves removing the sensitive variables such as `race` from the dataset.\n","The idea is that if the algorithm doesn't have access to these sensitive attributes, it cannot be biased against them.\n","\n"],"metadata":{"id":"hRrBNnjzsyIB"}},{"cell_type":"code","source":["y = data['high_risk']\n","\n","# One-hot encode other categorical columns, but exclude 'race'\n","X = pd.get_dummies(data.drop(['high_risk', 'race'], axis=1), drop_first=True)\n","X['race'] = data['race']"],"metadata":{"id":"VDEnKP5tMY4Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and test sets. Use as a test size 20% and random state =42 .\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","\n","# Drop race to obscure it from our model\n","X_train_with_race = X_train.copy() # make a copy of the X train set with race included\n","X_train = X_train_with_race.drop(['race'], axis=1)\n","\n","\n","X_test_with_race = X_test.copy() # make a copy of the X test set with the race included\n","X_test = X_test_with_race.drop(['race'], axis=1)"],"metadata":{"id":"FI538nBCQAWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train a simple machine learning model (Random Forest Classifier)\n","# ... first the full model\n","model_with_race = RandomForestClassifier(random_state=42)\n","model_with_race.fit(X_train_with_race, y_train)\n","y_pred_with_race = model_with_race.predict(X_test_with_race)\n","\n","# ... and then the model without the race attribute.\n","model = RandomForestClassifier(random_state=42)\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_test)"],"metadata":{"id":"eGRIiNurQJ7h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the accuracy of both models\n","baseline_accuracy = accuracy_score(y_test, y_pred_with_race)\n","unawareness_accuracy = accuracy_score(y_test, y_pred)\n","\n","# Calculate demographic parity difference for baseline model\n","dp_difference_baseline = demographic_parity_difference(y_test, y_pred_with_race, sensitive_features=X_test_with_race['race'])\n","\n","# Calculate demographic parity difference for unawareness model.\n","dp_difference = demographic_parity_difference(y_test, y_pred, sensitive_features=X_test_with_race['race'])\n","\n","\n","# Calculate equalized odds difference\n","eo_difference_baseline =  equalized_odds_difference(y_test, y_pred_with_race, sensitive_features=X_test_with_race['race'])\n","eo_difference =  equalized_odds_difference(y_test, y_pred, sensitive_features=X_test_with_race['race'])\n","\n","# Print results\n","\n","# Print Accuracy\n","print(f\"Baseline Model Accuracy: {baseline_accuracy:.4f}\")\n","print(f\"Unawareness Model Accuracy: {unawareness_accuracy:.4f}\")\n","print(40*\"-\")\n","\n","# Print Baseline Demographic parity difference.\n","print(f\"Baseline Demographic Parity Difference: {dp_difference_baseline:.4f}\")\n","print(f\"Unawareness Demographic Parity Difference: {dp_difference:.4f}\")\n","print(40*\"-\")\n","\n","# Print Baseline Equalized odds difference.\n","print(f\"Baseline Equalized odds Difference: {eo_difference_baseline:.4f}\")\n","print(f\"Unawareness Equalized odds Difference: {eo_difference:.4f}\")"],"metadata":{"id":"UNjG7EsWQRyP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Question***\n","\n","- Was this approach effective in removing racial bias?\n","- In what situations would **fairness through unawareness** not be effective?"],"metadata":{"id":"Z_DjddF8znvz"}},{"cell_type":"markdown","source":["# 2. In-Processing bias mitigation with Exponantiated Gradient\n","\n","Now we are going to try a bias mitigation algorithm that is offered by the **fairlearn** library. The algorithm is called **Exponantiated Gradient**\n","\n","\n","The Exponentiated Gradient method refines the model iteratively, adjusting sample weights to ensure fairness in decisions. It is using an iterative appraoch to do that:\n","\n","1. Initialization: All samples start with equal weights.\n","\n","2. Iterative Process:\n","\n"," - Train the model with the current weights.\n"," - Evaluate for fairness violations, e.g., disparate selection rates between African-Americans and Caucasians.\n"," - If unfairness is detected, increase the weights for samples that would help correct the fairness violation and decreases the weights for samples that would exacerbate it.\n","3. Convergence: Iterate until the model's decisions balance accuracy with fairness constraints.\n","\n","To do this we also need to provide the model with a fairness constraint that we want to optimize for, for this demonstration we will chose the demographic parity as the fairness constraint.\n","\n","*Note: Running the next cell may take a minute.*\n"],"metadata":{"id":"HLs6I-6mtMoh"}},{"cell_type":"code","source":["from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n","np.random.seed(42)  # set seed for consistent results with ExponentiatedGradient\n","\n","# Define the constraint\n","constraint = DemographicParity()\n","\n","# Select the classifier\n","classifier = RandomForestClassifier()\n","\n","# Construct the mitigator\n","mitigator = ExponentiatedGradient(classifier, constraint, max_iter=5)\n","race_train = X_train_with_race['race']\n","\n","# Train the random forest classifier with the Exponentiated Gradient mitigator\n","mitigator.fit(X_train_with_race, y_train, sensitive_features=race_train)\n","\n","# Now lets make predictions in the test set.\n","y_pred_mitigated = mitigator.predict(X_test_with_race)\n","\n","# Calculate the accuracy of the model with the fairness mitigation algorithm\n","exp_grad_accuracy = accuracy_score(y_test, y_pred_mitigated)\n","\n","# Calculate demographic parity difference\n","exp_grad_dp_difference = demographic_parity_difference(y_test, y_pred_mitigated, sensitive_features=X_test_with_race['race'])\n","\n","# Calculate equalized odds difference\n","exp_grad_eo_difference =  equalized_odds_difference(y_test, y_pred_mitigated, sensitive_features=X_test_with_race['race'])\n","\n","# Print results\n","print(f\"Baseline Model Accuracy: {baseline_accuracy:.4f}\")\n","print(f\"Exp. Grad. Model Accuracy: {exp_grad_accuracy:.4f}\")\n","print(40*\"-\")\n","\n","print(f\"Baseline Demographic Parity Difference: {dp_difference_baseline:.4f}\")\n","print(f\"Exp. Grad. Demographic Parity Difference: {exp_grad_dp_difference:.4f}\")\n","print(40*\"-\")\n","\n","print(f\"Baseline Equalized odds Difference: {eo_difference_baseline:.4f}\")\n","print(f\"Exp. Grad. Equalized odds Difference: {exp_grad_eo_difference:.4f}\")"],"metadata":{"id":"6gLYA8JQQ5-G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Question***\n","\n","- Which are your observations from a first glance at these results?\n","- How does this approach stack up against the **fairness through unawareness** method?"],"metadata":{"id":"Sc1ol_WStqsz"}},{"cell_type":"markdown","source":["Let's now dive a bit deeper into the metrics with visualizations as we did for the baseline model."],"metadata":{"id":"lsTmEHqTtuCX"}},{"cell_type":"code","source":["metric_frame_mitigated = MetricFrame(metrics=metrics,\n","                           y_true=y_test,\n","                           y_pred=y_pred_mitigated,\n","                           sensitive_features=race)\n","ax = metric_frame_mitigated.by_group.plot.bar(\n","    subplots=True,\n","    layout=[1, 3],\n","    legend=False,\n","    figsize=[15, 6],\n","    title=\"Show all metrics\",\n",")\n","# Remap the labels\n","label_mapping = {0: 'African-American', 1: 'Caucasian'}\n","\n","# Update x-tick labels for each subplot\n","for subplot in ax[0]:\n","    labels = [item.get_text() for item in subplot.get_xticklabels()]\n","    new_labels = [label_mapping[int(label)] if label.isdigit() else label for label in labels]\n","    subplot.set_xticklabels(new_labels, rotation=45)\n","\n","plt.tight_layout()"],"metadata":{"id":"dx14ocZ1Ssvo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Question***\n","\n","- How did these metrics change compared to the baseline?"],"metadata":{"id":"DvVv1MpYz8Ml"}},{"cell_type":"markdown","source":["# 3. Post-Processing bias mitigation with Threshold Optimizer\n","Another approach to mitigating bias is at the post-processing stage by altering the classification threshold for positive predictions.\n","\n","We will use the `ThresholdOptimizer` contained in the Fairlearn library to improve Demographic Parity.\n","\n","`ThresholdOptimizer` creates seperate classification thresholds for each race. It decides on the best classification thresholds by generating all possible thresholds and selecting the best compination in terms of the objective (in our case balanced_accuracy_score) and the fairness constraint (in our case demographic_parity).\n"],"metadata":{"id":"Co58Seo_UXRb"}},{"cell_type":"code","source":["from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer\n","np.random.seed(42)  # set seed for consistent results with ThresholdOptimizer\n","classifier = RandomForestClassifier()\n","race_train = X_train_with_race['race']\n","race_test = X_test_with_race['race']\n","\n","\n","threshold_optimizer = ThresholdOptimizer(\n","    estimator=classifier,\n","    constraints=\"demographic_parity\",\n","    objective=\"balanced_accuracy_score\",\n","    predict_method=\"predict_proba\",\n","    prefit=False\n","    )\n","\n","# Train the threshold optimizer\n","threshold_optimizer.fit(X_train_with_race, y_train, sensitive_features=race_train)\n","\n","# Now lets make predictions in the test set.\n","y_pred_mitigated = threshold_optimizer.predict(X_test_with_race, sensitive_features=race_test)\n","\n","# Calculate the accuracy of the model with the fairness mitigation algorithm\n","threshold_optimizer_accuracy = accuracy_score(y_test, y_pred_mitigated)\n","\n","# Calculate demographic parity difference\n","threshold_optimizer_dp_difference = demographic_parity_difference(y_test, y_pred_mitigated, sensitive_features=X_test_with_race['race'])\n","\n","# Calculate equalized odds difference\n","threshold_optimizer_eo_difference =  equalized_odds_difference(y_test, y_pred_mitigated, sensitive_features=X_test_with_race['race'])\n","\n","# Print results\n","print(f\"Baseline Model Accuracy: {baseline_accuracy:.4f}\")\n","print(f\"Threshold Optimizer Model Accuracy: {threshold_optimizer_accuracy:.4f}\")\n","print(40*\"-\")\n","\n","print(f\"Baseline Demographic Parity Difference: {dp_difference_baseline:.4f}\")\n","print(f\"Threshold Optimizer Demographic Parity Difference: {threshold_optimizer_dp_difference:.4f}\")\n","print(40*\"-\")\n","\n","print(f\"Baseline Equalized odds Difference: {eo_difference_baseline:.4f}\")\n","print(f\"Threshold Optimizer Equalized odds Difference: {threshold_optimizer_eo_difference:.4f}\")"],"metadata":{"id":"wBZDdOFaSD2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metric_frame_mitigated = MetricFrame(metrics=metrics,\n","                           y_true=y_test,\n","                           y_pred=y_pred_mitigated,\n","                           sensitive_features=race)\n","ax = metric_frame_mitigated.by_group.plot.bar(\n","    subplots=True,\n","    layout=[1, 3],\n","    legend=False,\n","    figsize=[15, 6],\n","    title=\"Show all metrics\",\n",")\n","# Remap the labels\n","label_mapping = {0: 'African-American', 1: 'Caucasian'}\n","\n","# Update x-tick labels for each subplot\n","for subplot in ax[0]:\n","    labels = [item.get_text() for item in subplot.get_xticklabels()]\n","    new_labels = [label_mapping[int(label)] if label.isdigit() else label for label in labels]\n","    subplot.set_xticklabels(new_labels, rotation=45)\n","\n","plt.tight_layout()"],"metadata":{"id":"5MXy0Ta0Td6K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ***Question***\n","\n","- How do the results stack up against the previous two bias mitigation strategies?"],"metadata":{"id":"mcZ711VZW7-v"}},{"cell_type":"code","source":[],"metadata":{"id":"FD3VZza5U6NW"},"execution_count":null,"outputs":[]}]}