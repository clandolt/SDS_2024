{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Get used to the OpenAI API\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai tiktoken sentence_transformers pandas numpy python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from typing import List, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv  \n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Let's set up some constants. These are:\n",
    "\n",
    "- The model we will be using (defined in Azure OpenAI).\n",
    "- Cost for token completion and prompt (we take this from Azure OpenAI Studio for the respective model).\n",
    "- Openai API setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = os.environ.get(\"OPENAI_API_DEPLOYMENT\", \"gpt-4\")\n",
    "MODEL = \"gpt-35-turbo\"  # Uncomment if token rate is too high\n",
    "\n",
    "%run model_cost.py\n",
    "model_cost = get_model_instance(model_name=MODEL)\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_VERSION\", \"2023-07-01-preview\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ.get(\"AZURE_OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a prompt and receive response from the API\n",
    "\n",
    "We will send a prompt to the API and get a response back. The prompt is a string that is used to \"seed\" the model. The model will then generate a completion based on the prompt. The completion is a string of text that is generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(\n",
    "        prompt: List[Dict[str, str]],\n",
    "        model: str = MODEL\n",
    "    ) -> openai.types.chat.chat_completion.ChatCompletion:\n",
    "    \"\"\"Function to ask a question to the GPT model using the Azure OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the GPT model\n",
    "        model: The model to use\n",
    "\n",
    "    Returns:\n",
    "        The response from the GPT model\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1500,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "    )\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to monitor costs. We will do this with the function `get_cost()`. This function will return the cost of the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(\n",
    "        response: openai.types.chat.chat_completion.ChatCompletion,\n",
    "        token_cost_per_completion: float,\n",
    "        token_cost_per_prompt: float,\n",
    "    ) -> float:\n",
    "    \"\"\"Function to compute the cost of a prompt + completion response.\n",
    "    \n",
    "    Args:\n",
    "        response: The response from the GPT model\n",
    "        token_cost_per_completion: The cost per completion token\n",
    "        token_cost_per_prompt: The cost per prompt token\n",
    "    \n",
    "    Returns:\n",
    "        The cost of the prompt + completion response\n",
    "    \"\"\"\n",
    "    completion_tokens = response.usage.completion_tokens\n",
    "    prompt_tokens = response.usage.prompt_tokens\n",
    "\n",
    "    cost = (completion_tokens * token_cost_per_completion) + (\n",
    "        prompt_tokens * token_cost_per_prompt\n",
    "    )\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a question to ask to the model\n",
    "question = \"Which one is the best Greek Island for a vacation?\"\n",
    "\n",
    "# Set the character\n",
    "character = \"You answer question about the Greek Islands. Include a joke about Greece in every response.\"\n",
    "\n",
    "# Build the prompt\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": character\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    },\n",
    "]\n",
    "\n",
    "# Ask the question to the model\n",
    "response = ask_question(prompt=prompt)\n",
    "\n",
    "# Display the answer\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))\n",
    "\n",
    "# Compute and print the cost\n",
    "cost = get_cost(\n",
    "    response=response,\n",
    "    token_cost_per_completion=model_cost.token_cost_per_completion,\n",
    "    token_cost_per_prompt=model_cost.token_cost_per_prompt\n",
    ")\n",
    "print(f\"Total cost for this response: {cost:.5f} {model_cost.currency} (model was {model_cost.name}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Integrating Private Domain Knowledge\n",
    "\n",
    "Note that you need to have executed all cells in Part 1 for Part 2 to function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level workflow description\n",
    "\n",
    "<img src=\"images/0_overview.png\" alt=\"Image description\" width=\"1000\">  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Travel Articles and Calculate the Embeddings\n",
    "\n",
    "We have prepared the travel articles for you. These articles constitute private domain knowledge that the model has no access to. We will load them into memory and calculate the embeddings for each article. We will use these embeddings to retrieve the most relevant articles for a given question.\n",
    "\n",
    "<img src=\"images/1_calculate_domain_knowledge.png\" alt=\"Image description\" width=\"200\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to provide the path to the travel articles to the Notebooks local instance and provide the path to the travel articles in the variable `path_travel_articles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the path to the embeddings file as a string, e.g., path_travel_articles = \"./data/travel_articles/\"\n",
    "path_travel_articles = \"./data/travel_articles/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each article is in a separate text file. The code snipped below will discover all the text files in the directory and load them into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_articles = []\n",
    "for file in os.listdir(path_travel_articles):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(path_travel_articles, file), \"r\") as f:\n",
    "            travel_articles.append(f.read())\n",
    "\n",
    "display(Markdown(travel_articles[17]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the embeddings using the sentence transformer model `all-mpnet-base-v2` (https://huggingface.co/sentence-transformers/all-mpnet-base-v2). Observe that all the vectors are of the same size, `768`. This is because the transformer model outputs a fixed-size vector for each input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "embeddings = []\n",
    "for article in travel_articles:\n",
    "    embedding = embedding_model.encode(article, show_progress_bar=True)\n",
    "    embeddings.append(embedding)\n",
    "    print(f\"Shape of the vector: {embedding.shape}\")\n",
    "\n",
    "embeddings[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define User Request\n",
    "\n",
    "We will define a user request. This is a question that we would like to ask the model. We will calculate the embedding for this question and use it to retrieve the most relevant articles from the travel articles by comparing the vector of the User Request to the other vectors we just calculated above.\n",
    "\n",
    "<img src=\"images/2_define_UR.png\" alt=\"Image description\" width=\"200\">  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the User Request as a string, e.g., user_request = \"I want to travel to a Greek Island that is famous this season for snorkeling.\"\n",
    "user_request = \"I want to travel to a Greek Island that is famous this season for snorkeling. Which one would you recommend to me?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the embeddings for the User Request\n",
    "\n",
    "In the same fashion as before, we calculate the embeddings for the User Request using the same sentence transformer model that we used before - `all-mpnet-base-v2`.\n",
    "\n",
    "The vector will have the same shape as the vectors we calculated for the travel articles. Had we used a different model, the vectors would likely have been of a different size. In any event, this would have made it impossible to compare the vectors, we would compare apples to oranges.\n",
    "\n",
    "<img src=\"images/3_calculate_embeddings.png\" alt=\"Image description\" width=\"200\">  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_request_embedding = embedding_model.encode(user_request)\n",
    "\n",
    "print(f\"Shape of the user request vector: {user_request_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the N most similar articles\n",
    "\n",
    "We will compare the User Request vector to the vectors of the travel articles. There are different ways to compare vectors. For simplicity, we will use the cosine similarity.\n",
    "\n",
    "In principle, we could select any number N of articles. To honor token limits (globally for the endpoint we are using and locally in each request), we will select the top 3 articles.\n",
    "\n",
    "<img src=\"images/4_select_n.png\" alt=\"Image description\" width=\"200\">  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_between_vector_and_vectors(\n",
    "    vector: np.ndarray, vectors_array: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarities between a single vector and an array of vectors.\n",
    "\n",
    "    Args:\n",
    "        vector: A single vector\n",
    "        vectors_array: An array of vectors\n",
    "    \n",
    "    Returns:\n",
    "        An array of cosine similarities\n",
    "    \"\"\"\n",
    "    dot_products = np.dot(vectors_array, vector)\n",
    "\n",
    "    # Calculate the magnitudes of all vectors in vectors_array\n",
    "    magnitudes = np.sqrt(np.sum(np.square(vectors_array), axis=1))\n",
    "\n",
    "    # Calculate the magnitude of vector\n",
    "    magnitude_1 = np.linalg.norm(vector)\n",
    "\n",
    "    # Calculate the cosine similarities between vector and all vectors in vectors_array\n",
    "    similarities = dot_products / (magnitude_1 * magnitudes)\n",
    "\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3 # You can increase this number, but you might run into a token limit\n",
    "\n",
    "distances = distance_between_vector_and_vectors(\n",
    "    vector=user_request_embedding,\n",
    "    vectors_array=embeddings\n",
    ")\n",
    "\n",
    "idx_of_most_similar = list((-distances).argsort()[:k])\n",
    "\n",
    "print(f\"Most similar articles are articles numbers: {idx_of_most_similar} with similarities: {distances[idx_of_most_similar]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the text corresponding to the most similar articles embeddings\n",
    "\n",
    "We will retrieve the text of the most similar articles. We do this by looking up the index of the most similar articles in the list of travel articles.\n",
    "\n",
    "Check if the articles make sense and are relevant to the user request that you defined.\n",
    "\n",
    "<img src=\"images/5_retrieve_text.png\" alt=\"Image description\" width=\"200\">  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_articles = [travel_articles[i] for i in idx_of_most_similar]\n",
    "\n",
    "for i, article in enumerate(most_similar_articles):\n",
    "    print(f\"Article {i+1}\")\n",
    "    display(Markdown(article[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the Prompt with the retrieved text\n",
    "\n",
    "We will augment the prompt with the retrieved text. This will provide the model with the private domain knowledge it would otherwise not know about and that it can use to generate a more accurate response.\n",
    "\n",
    "<img src=\"images/6_augment_prompt.png\" alt=\"Image description\" width=\"200\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the character (system message) below as a string. \n",
    "\n",
    "Set the character (system message) below as a string. \n",
    "\n",
    "When using the OpenAI API, both setting a system message and including the character in the prompt will serve to guide the model's behavior.\n",
    "A system message is often used with OpenAI's Chat API, where the conversation occurs in a more interactive and dynamic manner. By using a system message, you can provide instructions or context to the AI model. This message isn't counted towards the token limit. However, it may have less influence on the response compared to including the character directly in the prompt.\n",
    "\n",
    "Some example system messages are:\n",
    "- You are a marketing writing assistant. You help come up with creative content ideas and content like marketing emails, blog posts, tweets, ad copy and product descriptions. You write in a friendly yet professional tone but can tailor your writing style that best works for a user-specified audience. If you do not know the answer to a question, respond by saying \"I do not know the answer to your question.\"\n",
    "- Assistant is an AI chatbot that helps users turn a natural language list into JSON format. After users input a list they want in JSON format, it will provide suggested list of attribute labels if the user has not provided any, then ask the user to confirm them before creating the list.\n",
    "- You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the character\n",
    "character = \"You are a seasoned travel agent with the primary goal to help users looking to plan a vacation in Greece. You write in a friendly yet professional tone.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to build the question to ask the model enriched with the information from the embeddings.\n",
    "\n",
    "Below you find an example how the augmented prompt could look like (using the `user request` that you defined). You can use this as a template to build your own question if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_instruction = 'Use the below travel articles about Greek Islands from the current travel season to answer \\\n",
    "the subsequent question. If the answer cannot be found in the articles, write \"I could \\\n",
    "not find an answer.\"'\n",
    "augmented_prompt = f\"{context_instruction} \\n\\n Question: {user_request} \\n\\n The travel articles follow below: \\n\\n\"\n",
    "\n",
    "for piece in most_similar_articles:\n",
    "    augmented_prompt += piece\n",
    "\n",
    "# Uncomment the below line to see the result of the example\n",
    "display(Markdown(augmented_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send the augmented prompt to the LLM\n",
    "\n",
    "We will send the augmented prompt to the LLM and get a response back. The response will be the completion generated by the model.\n",
    "\n",
    "<img src=\"images/7_to_llm.png\" alt=\"Image description\" width=\"200\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the prompt \n",
    "\n",
    "The OpenAI API expects the \"prompt\" to be in a specific form:\n",
    "> prompt = [ \\\n",
    ">    {\"role\": \"system\", \"content\": character goes here}, \\\n",
    ">    {\"role\": \"user\", \"content\": the question you wish to ask goes here}, \\\n",
    ">] \n",
    "\n",
    "We build the prompt below referring to the system message and augmented prompt that you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": character\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": augmented_prompt\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(\n",
    "        prompt: List[Dict[str, str]],\n",
    "        model: str = MODEL\n",
    "    ) -> openai.types.chat.chat_completion.ChatCompletion:\n",
    "    \"\"\"Function to ask a question to the GPT model using the Azure OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the GPT model\n",
    "        model: The model to use\n",
    "\n",
    "    Returns:\n",
    "        The response from the GPT model\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=prompt,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1500,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "def get_cost(\n",
    "        response: openai.types.chat.chat_completion.ChatCompletion,\n",
    "        token_cost_per_completion: float,\n",
    "        token_cost_per_prompt: float,\n",
    "    ) -> float:\n",
    "    \"\"\"Function to compute the cost of a prompt + completion response.\n",
    "    \n",
    "    Args:\n",
    "        response: The response from the GPT model\n",
    "        token_cost_per_completion: The cost per completion token\n",
    "        token_cost_per_prompt: The cost per prompt token\n",
    "    \n",
    "    Returns:\n",
    "        The cost of the prompt + completion response\n",
    "    \"\"\"\n",
    "    completion_tokens = response.usage.completion_tokens\n",
    "    prompt_tokens = response.usage.prompt_tokens\n",
    "\n",
    "    cost = (completion_tokens * token_cost_per_completion) + (\n",
    "        prompt_tokens * token_cost_per_prompt\n",
    "    )\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `ask_question` to use the Azure OpenAI API to interact with the model\n",
    "\n",
    "response = ask_question(\n",
    "    prompt=prompt,\n",
    "    model=MODEL\n",
    ")\n",
    "\n",
    "# Consider also the cost\n",
    "cost = get_cost(\n",
    "    response=response,\n",
    "    token_cost_per_completion=model_cost.token_cost_per_completion,\n",
    "    token_cost_per_prompt=model_cost.token_cost_per_prompt\n",
    ")\n",
    "\n",
    "print(f\"Total cost for this response: {cost:.5f} {model_cost.currency} (model was {model_cost.name}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the answer from the model nicely formatted\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You augmented the prompt with the retrieved text from the travel articles. You can use this approach to retrieve information from any text collection that you have. You can also use this approach to retrieve information from a collection of documents that you have in your company. For example, you could use this approach to retrieve information from your company's internal wiki or your own E-Mail inbox (if you're aware that you will be sending all the data to the model).\n",
    "\n",
    "However, as usual, be aware of the privacy and security implications of sending data to the model - Think before you hit send.\n",
    "\n",
    "If you want to further explore and try out different questions to ask, you can use the methods that we provided for you below in the \"Talk to GPT\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk to GPT\n",
    "\n",
    "Using the steps above, you can use below cell to play around with the OpenAI API and the travel articles we provided. You can ask the model any question you like.\n",
    "\n",
    "In the cell directly below we define a few helper functions to make your life easier. You can use them to send a prompt to GPT and display the result. The methods are the same as above, you have to copy your work down, for example for the system message and the augmented prompt.\n",
    "\n",
    "You will need to have loaded the travel articles and calculated the embeddings for them in order to use the methods below.\n",
    "\n",
    "Try to test the limits, e.g., asking about an activity like \"walking on the moon\", or asking about a location that is not in the travel articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_between_vector_and_vectors(\n",
    "    vector: np.ndarray, vectors_array: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarities between a single vector and an array of vectors.\n",
    "\n",
    "    Args:\n",
    "        vector: A single vector\n",
    "        vectors_array: An array of vectors\n",
    "    \n",
    "    Returns:\n",
    "        An array of cosine similarities\n",
    "    \"\"\"\n",
    "    dot_products = np.dot(vectors_array, vector)\n",
    "\n",
    "    # Calculate the magnitudes of all vectors in vectors_array\n",
    "    magnitudes = np.sqrt(np.sum(np.square(vectors_array), axis=1))\n",
    "\n",
    "    # Calculate the magnitude of vector\n",
    "    magnitude_1 = np.linalg.norm(vector)\n",
    "\n",
    "    # Calculate the cosine similarities between vector and all vectors in vectors_array\n",
    "    similarities = dot_products / (magnitude_1 * magnitudes)\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_pieces_of_interest(question: str, k: int = 3) -> List[str]:\n",
    "    \"\"\"Function to get the pieces of interest for a question.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask the model\n",
    "        k: The number of pieces of interest to find\n",
    "    \n",
    "    Returns:\n",
    "        The pieces of interest\n",
    "    \"\"\"\n",
    "    question_embedding = embedding_model.encode(question, show_progress_bar=True)\n",
    "\n",
    "    distances = distance_between_vector_and_vectors(\n",
    "        vector=question_embedding,\n",
    "        vectors_array=embeddings\n",
    "    )\n",
    "\n",
    "    idx_of_interest = list((-distances).argsort()[:k])\n",
    "    pieces_of_interest = [travel_articles[i] for i in idx_of_interest]\n",
    "\n",
    "    return pieces_of_interest\n",
    "\n",
    "\n",
    "def get_augmented_prompt(\n",
    "        question: str,\n",
    "        pieces_of_interest: List[str],\n",
    "        context_instruction: str,\n",
    "    ) -> str:\n",
    "    \"\"\"Function to build the augmented prompt.\n",
    "    \n",
    "    Args:\n",
    "        question: The question to ask the model\n",
    "        pieces_of_interest: The pieces of interest\n",
    "        context_instruction: The context instruction\n",
    "    \n",
    "    Returns:\n",
    "        The augmented prompt\n",
    "    \"\"\"\n",
    "    augmented_prompt = f\"{context_instruction} \\n\\n Question: {question} \\n\\n The travel articles follow below: \\n\\n\"\n",
    "\n",
    "    for piece in pieces_of_interest:\n",
    "        augmented_prompt += piece\n",
    "\n",
    "    return augmented_prompt\n",
    "\n",
    "# Define a user request\n",
    "user_request = \"I want to travel to a Greek Island that is famous this season for eating good food. Which one would you recommend to me?\"\n",
    "\n",
    "# Select k, the number of pieces of interest\n",
    "k = 3\n",
    "\n",
    "pieces_of_interest = get_pieces_of_interest(question=user_request, k=k)\n",
    "\n",
    "# Set the character\n",
    "character = \"You are a seasoned travel agent with the primary goal to help users looking to plan a vacation in Greece. You write in a friendly yet professional tone.\"\n",
    "\n",
    "# Provide your augmented prompt\n",
    "context_instruction = 'Use the below travel articles about Greek Islands from the current travel season to answer \\\n",
    "the subsequent question. If the answer cannot be found in the articles, write \"I could \\\n",
    "not find an answer.\"'\n",
    "augmented_prompt = get_augmented_prompt(\n",
    "        question=user_request,\n",
    "        pieces_of_interest=pieces_of_interest,\n",
    "        context_instruction=context_instruction,\n",
    ")\n",
    "\n",
    "# Build the prompt\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": character\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": augmented_prompt\n",
    "    },\n",
    "]\n",
    "\n",
    "# Send the prompt to the model\n",
    "response = ask_question(\n",
    "    prompt=prompt,\n",
    "    model=MODEL\n",
    ")\n",
    "\n",
    "# Consider the cost\n",
    "cost = get_cost(\n",
    "    response=response,\n",
    "    token_cost_per_completion=model_cost.token_cost_per_completion,\n",
    "    token_cost_per_prompt=model_cost.token_cost_per_prompt\n",
    ")\n",
    "print(f\"Total cost for this response: {cost:.5f} {model_cost.currency} (model was {model_cost.name}).\")\n",
    "\n",
    "\n",
    "# Display the answer from the model nicely formatted\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
